# Section 3.4: Named Discrete Distributions 

Now that the foundations of random variables, probability distributions expectation and variance are under our belt, let's start to look at some special random variables that occur so commonly, or have such mathematically wonderful properties that they have specific names. We will look at 6 different types of discrete random variables. For each we will learn the following:

* How to define the random variable
* How to identify the parameters and write the distributional notation
* The formula for the pmf, and how to find theoretical probabilities
* Formulas for the theoretical mean and variance
* How to calculate all of the above using R commands (both theoretical, and via simulation)


### A note on the R commands

In R, the common distributions are defined by their **root** name with 3 different prefixes:

* `d` to compute $P(X == x)$ e.g.: `dbinom, dgeom, dhyper, dnbinom`
* `p` to compute $P(X\leq x)$ e.g.: `pbinom, pgeom, phyper, pnbinom`
* `r` to randomly draw N samples from the specified distribution. e.g: `rbinom, rgeom, rhyper, rnbinom`


\newpage

## Bernoulli Distribution (Speegle 3.3)
**Situation** The simplest type of experiment is one in which there are only two outcomes (success/failure, live/die, true/false, yes/no etc.). When running simulations in Chapter 2, you wrote your experiment to get down to a single TRUE/FALSE. You were creating a Bernoulli random variable. This simple, yet fundamental random variable serves as the basis for the rest of the distributions in this chapter. 

**Random variable:** Let $X$ be a random variable that denotes the outcome from a Bernoulli trial with probability of success $p$. Specifically let $X=1$ denote a success, and $X=0$ denote a failure. (What is considered a _success_ is entirely up to context. If you are interested in mortality rate for a certain disease, then "death" would be a success.)

**Distributional Notation:** $X \sim Bernoulli(p)$

**pmf:** $P(X=x) = p^{x}(1-p)^{1-x} \qquad x\geq 0$

**Mean and variance:** $E(X) = p \qquad Var(X) = p(1-p)$

**R commands:** There are no fancy named R commands for this distribution. You can simulate this random variable using `sample(c(0,1), prob=c(1-p, p))` directly, or through a Binomial random variable with $n$=1. 


### Example:  
A beet seed has been planted, and will either germinate or not. The probability of germination is 0.8, and germination is considered a success. 

### You try it: 
Define 2 Bernoulli trials. 


\newpage
## Binomial Distribution (Speegle 3.3.1)

**Situation:** If $n$ independent random variables $X_{1},...,X_{n}$ all have the same Bernoulli distribution with probability of success $p$, then their sum is equal to the number of $X_{i}$'s which equal 1, and the distribution of the sum is known as a Binomial distribution. Examples include: 

* Toss 5 coins and count the number of heads
* The number of times in a week a person is late for work, whey they have a 10% chance of being late each day, independent of other days. 

**Random variable:** Let $X$ be a random variable that represents the number of "success" in a series of $n$ independent Bernoulli trials each with probability success $p$. 

**Distributional Notation:** $X \sim Binomial(n, p)$

**pmf:** 
$$
P(X = x)= \binom{n}{x}p^{x}(1-p)^{n-x} \qquad x=0,1,2,...,n
$$

**Mean and variance:** $E(X) = np \qquad Var(X) = np(1-p)$

\singlespace

**R commands:**  

* `dbinom(x, size=n, prob=p)` to compute $P(X == x)$ 
* `pbinom(x, size=n, prob=p)` to compute $P(X\leq x)$ 
* `rbinom(N, size=n ,prob=p)` to randomly draw N samples from a $X \sim Binom(n, p)$ distribution. 

\vspace{0.5cm}

**Visualizing the shape of the distribution:**
```{r, fig.width=10, fig.height=3, fig.align='center', echo=FALSE}
par(mfrow=c(1,3))
plot(proportions(table(rbinom(1000, 10, .1))), main="n=10, p=.1", ylab="p(x)")
plot(proportions(table(rbinom(1000, 10, .5))), main="n=10, p=.5", ylab="p(x)")
plot(proportions(table(rbinom(1000, 10, .9))), main="n=10, p=.9", ylab="p(x)")
```
What happens to the distribution as p increases? 
\doublespace

\newpage

### Example:  
1. Plant 10 beet seeds, and assume that the germination of one seed is independent of the germination of another seed, and all seeds have a germination probability of $p$=.8. Let $X$ be the number of seeds that germinated. 1) Write down the pmf, and 2) the distributional notation for $X$. Then compute the mean and variance both 3) theoretically, and 4) confirm using simulation. 

\vspace{2cm}

**Theoretical**

\vspace{2cm}

**Simulation**

\vspace{2cm}

2. A coin for which the probability of heads is .6 is tossed nine times. Find the probability of obtaining 3 heads. 

**by hand using the pmf**

\vspace{2cm}

**theoretical using R commands**

\vspace{2cm}

**using simulation**

\vspace{2cm}

3. 10 students are selected at random, each has a probability of 0.10 of being a Math major. What is the probability that at least one student is a math major? 

**by hand using the pmf**

\vspace{4cm}

**theoretical using R commands**

\vspace{2cm}

**using simulation**

\vspace{2cm}

\newpage

### You try it:  
1. A recent national study showed that approximately 45\% of college students binge drink.  Let $X$ equal the number of students in a random sample of size $n=12$ who binge drink. Calculate the following probabilities both by hand using the pmf, and R commands.

a. $X$ is at most 2.

\vspace{3cm}
b. $X$ is at least 1.
\vspace{3cm}

c. Use simulation to obtain the mean and variance of $X$. 
\vspace{3cm}

2. A certain electric system contains 10 components.  Suppose that the probability that each individual will fail is .2 and that the components fail independently of each other. **Given that at least one of the components failed**, what is the probability that at least two of the components have failed? Write this out mathematically and simplify _before_ you go to R. 


\newpage
## Geometric Distribution (Speegle 3.3.2)

**Situation** Given a series of independent Bernoulli trials, we are accustomed to thinking of $n$ and $p$ as fixed, and $x$ is considered the number of successes for a binomial distribution.  Suppose that the problem is turned around though, and the question is asked, how many trials will be required in order to achieve the first success?  Put this way, the number of trials is the random variable and number of successes is fixed. 

* How many free throws can Stephen Curry make before he misses? 
* The probability that a random person who smokes will develop a severe lung condition in their lifetime is about 0.3. How many people do you have to check on before you meet someone with a severe lung condition? 

**Random Variable:** Let $X$ be the number of failures before the first success in a Bernoulli process with probability of success $p$.

**Distributional Notation:** $X \sim Geom(p)$

**pmf:** 
$$
P(X = x)= (1-p)^{x}p \qquad x=0,1,2,...
$$

**Mean and variance:** $E(X) = \frac{1-p}{p} \qquad Var(X) = \frac{1-p}{p^{2}}$

\singlespace
**R commands:** 

* `dgeom(x,prob=p)` to compute $P(X == x)$ 
* `pgeom(x,prob=p)` to compute $P(X\leq x)$ 
* `rgeom(N,prob=p)` to randomly draw N samples from a $X \sim Geom(p)$ distribution.

\vspace{0.5cm}

**Visualizing the shape of the distribution**
```{r, fig.width=10, fig.height=3, fig.align='center', echo=FALSE}
par(mfrow=c(1,3))
plot(proportions(table(rgeom(1000, .1))), main="p=.1", ylab="p(x)")
plot(proportions(table(rgeom(1000, .5))), main="p=.5", ylab="p(x)")
plot(proportions(table(rgeom(1000, .9))), main="p=.9", ylab="p(x)")
```
What happens to the distribution as p increases? 
\doublespace

\newpage

### Example
Professional basketball player Steve Nash was a 90% free throw shooter over his career. Answer the following questions using the formulas and also simulation. 

a. If Steve Nash starts shooting free throws, how many would he expect to make before missing one? 

**Theoretical**

\vspace{3cm}

**Simulation**

\vspace{2cm}

b. What is the probability that he could make 20 in a row before he misses? 

**by hand using the pmf**

\vspace{3cm}

**theoretical using R commands**

\vspace{2cm}

**using simulation**

\vspace{2cm}


### You try it: 
_Complete the following using both theoretical and simulation methods_

1. The 2010 American Community Survey estimates that 47.1% of women ages 15 years and over are married. We randomly select three women between these ages. 

a. What is the probability that the third women selected is the only one that is married? 

\vspace{3cm}

b. On average, how many women would you expect to sample before selecting a married woman? What is the standard deviation? 
\vspace{3cm}

2. A machine that produces a special type of transistor has a 2% defective rate. The production is considered a random process where each transistor is independent of the others. 

a. What is the probability that the 10th transistor produced is the first with a defect? \vspace{3cm}

b. What is the probability that the first failure occurs after the 4th transistor was produced? 

\newpage

## Negative Binomial Distribution (Speegle 3.6.2)
**Situation** A random variable with a negative binomial distribution originates from a context much like the one that yields the geometric distribution. Again, we focus on independent and identical trials, each of which results in one of two outcomes, success or failure. The probability of success, $p$, stays constant for each trial. The geometric case handles the number of cases until the first success occurs. What if we are interested in knowing the number of trials until the second, third, fourth, etc success occurs. Examples: 

* How many people do you have to meet at college before you meet the 4th person from your hometown?
* Tire Mart has a lot of really cheap tires, but 20% of them are defective. How many tires do you need to go through to find 4 new tires?

**Random Variable:** Let $X$ denotes the number of failures before the $n$th success, with probability of success $p$.

**Distributional Notation:** $X \sim NegBin(n, p)$

**pmf:** 
$$
P(X = x)=\binom{x+n-1}{x}p^{n}(1-p)^{x} \qquad x=0,1,2,...
$$

We can think of the negative binomial distribution as the sum of $r$ geometric distributions. This simplifes the **Mean and variance:** $E(X) = \frac{n(1-p)}{p} \qquad Var(X) = \frac{n(1-p)}{p^{2}}$

\singlespace
**R commands:** 

* `dnbinom(x, n, prob=p)` to compute $P(X == x)$ 
* `pnbinom(x, n, prob=p)` to compute $P(X\leq x)$ 
* `rnbinom(N, n, prob=p)` to randomly draw N samples from a $X \sim NegBin(n, p)$ distribution.

\vspace{0.5cm}

**Visualizing the shape of the distribution:**
```{r, fig.width=10, fig.height=3, fig.align='center', echo=FALSE}
par(mfrow=c(1,3))
plot(proportions(table(rnbinom(1000, 2, .5))), main="n=2, p=.5", ylab="p(x)")
plot(proportions(table(rnbinom(1000, 5, .5))), main="n=5, p=.5", ylab="p(x)")
plot(proportions(table(rnbinom(1000, 5, .9))), main="n=5, p=.9", ylab="p(x)")
```
What happens to the distribution as n and p change? 
\doublespace

\newpage

### Example: Oil!
A geological study indicates that an exploratory oil well drilled in a particular region should strike oil with probability 0.2.  Write down the pmf, and then calculate the mean and variance. 

**Theoretical**

\vspace{3cm}

**Simulation**

\vspace{2cm}

Find the probability that the third oil strike comes on the fifth well drilled.


**by hand using the pmf**

\vspace{3cm}

**theoretical using R commands**

\vspace{2cm}

**using simulation**

\vspace{2cm}

\newpage

### You try it: 
Ten percent of the engines manufactured on an assembly line are defective. 

* If engines are randomly selected one at a time and tested, what is the probability that the first non defective engine will be found on the second trial?

\vspace{3cm}

* What is the probability that the third non defective engine will be found on the fifth trial?

\vspace{3cm}
* Find the mean and variance of the number of trials on which the first non defective engine is found.

\vspace{3cm}
* Find the mean and variance of the number of failures until the third non defective engine is found. 

\newpage

## Poisson Distribution (Speegle 3.6.1)
**Situation:** A Poisson process is one where events occur at random times during a fixed time period. The events occur independently from each other, but with a constant average rate over that time period. Examples include

* Number of calls per hour at a call center
* Number of hits on a webpage in a day
* Number of meteor strikes on the surface of the moon annually

**Random Variable:** Let $X$ be the number of events occurring in a Poisson process with rate $\lambda$ over one unit of time (e.g. per year, per second, per day). 

**Distributional Notation:** $X \sim Poisson(\lambda)$

**pmf:** 
$$
P(X = x) = e^{-\lambda}\frac{\lambda^{x}}{x!}
$$

**Mean and variance:** $E(X) = Var(X) = \lambda$

\singlespace
**R commands:** 

* `dpois(x,lambda)` to compute $P(X == x)$ 
* `ppois(x,lambda)` to compute $P(X\leq x)$ 
* `rpois(N,lambda)` to randomly draw N samples from a $X \sim Poisson(\lambda)$ distribution.

\vspace{0.5cm}

**Visualizing the shape of the distribution**
```{r, fig.width=10, fig.height=3, fig.align='center', echo=FALSE}
par(mfrow=c(1,3))
plot(proportions(table(rpois(1000, .1))), main="lambda = 0.1", ylab="p(x)")
plot(proportions(table(rpois(1000, 1))), main="lambda = 1", ylab="p(x)")
plot(proportions(table(rpois(1000, 10))), main="lambda = 10", ylab="p(x)")
```
What happens to the distribution as $\lambda$ increases? 
\doublespace

\newpage

### Example
The Taurids meteor shower is visible on clear nights in the Fall and can have visible meteor rates around five per hour. What is the probability that a viewer will observe exactly eight meteors in two hours? 


**by hand using the pmf**

\vspace{3cm}

**theoretical using R commands**

\vspace{2cm}

**using simulation**

\vspace{2cm}


### You try it:
Suppose a typist makes typos at a rate of 3 typos per 10 pages. What is the probability that they will make at most one typo on a five page document?

\newpage

## Hypergeometric Distribution (Speegle 3.6.3)

**Situation:** The hypergeometric distribution is a series of Bernoulli trials that are dependent. This occurs when we are *sampling without replacement* from a finite population. Examples include: 

* Capture some fish, tag them & release them. Then come back later and fish some more, counting how many tagged ones you catch again. 
* Create a bipartisan committee of 10 senators, and count the number of Republicans chosen. 

**Random Variable:** Let $X$ denote the number of success out of a sample size of $k$ when drawing without replacement from a pool where there are a total of $m$ successes and $n$ failures available.

**Distributional Notation:** $X \sim Hypergeometric(m+n, n, k)$

\singlespace

**pmf:** 

$$
P(X=x)=\frac{\binom{m}{x}\binom{n}{k-x}}{\binom{m+n}{k}}
$$

**Mean and variance:** 
$$
E[X]=k\left(\frac{m}{m+n}\right) \qquad \qquad 
V(X)=k\left(\frac{m}{m+n}\right)\left(\frac{n}{m+n}\right)\left(\frac{m+n-k}{m+n-1}\right)
$$

**R commands:** 

* `dhyper(x, m, n, k)` to compute $P(X == x)$ 
* `phyper(x, m, n, k)` to compute $P(X\leq x)$ 
* `rhyper(N, m, n, k)` to randomly draw N samples from a $X \sim Hypergeometric(m+n, n, k)$ distribution.

\vspace{0.5cm}

**Visualizing the shape of the distribution**
```{r, fig.width=10, fig.height=3, fig.align='center', echo=FALSE}
par(mfrow=c(1,3))
plot(proportions(table(rhyper(1000, 10, 10, 4))), main="m=10, n=10, k=4", ylab="p(x)")
plot(proportions(table(rhyper(1000, 5, 10, 4))), main="m=5, n=10, k=4", ylab="p(x)")
plot(proportions(table(rhyper(1000, 10, 5, 4))), main="m=10, n=5, k=4", ylab="p(x)")
```
What happens to the distribution as the parameters change? 
\doublespace
\newpage

### Example
1. An urn contains nine chips, five red and four white.  Three are drawn out at random without replacement. Let $X$ denote the number of red chips in the sample. Identify the parameters, and find $E(X)$ and $Var(X)$.

**Theoretical**

\vspace{3cm}

**Simulation**

\vspace{2cm}


2. In a small pond there are 50 fish, 10 of which have been tagged.  If a fisherman's catch consists of 7 fish, selected at random and without replacement, and $X$ denotes the number of tagged fish, what is the probability that exactly 2 tagged fish are caught?
  
  
**by hand using the pmf**
  
\vspace{3cm}

**theoretical using R commands**
  
\vspace{2cm}

**using simulation**
  
\vspace{2cm}


### You try it:
1. Suppose that there are 3 defective items in a lot of 50 items.  A sample of size 10 is taken at random and without replacement.  Let $X$ denote the number of defective items in the sample.  Find the probability that the sample contains
a. Exactly 1 defective item. \vspace{3cm}   
b. At most 1 defective item.

\vspace{3cm}


2. A display case contains thirty-five diamonds, of which ten are real diamonds and twenty-five are fake diamonds. A burglar removes four gems at random, one at a time and without replacement.  What is the probability that the last gem she steals is the second real diamond in the set of four?


\newpage
Additional notes.
